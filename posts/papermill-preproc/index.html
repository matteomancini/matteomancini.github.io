<!doctype html>
<html lang="en-uk">
  <head>
    <title>Grind those brains! Diffusion MRI preprocessing with papermill // NeuroSnippets</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.76.5" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Matteo Mancini" />
    <meta name="description" content="" />
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">


    <link rel="stylesheet" href="https://neurosnippets.com/css/main.min.9594d69abdd169534d699b3e364130681441a142ca55039d76d678b2eb7141b8.css" />

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-187589071-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Grind those brains! Diffusion MRI preprocessing with papermill"/>
<meta name="twitter:description" content="Notebooks: you love them or you hate them Notebooks are an interesting beast: they have become extremely diverse and popular, everyone loves them, from declared non-programmers to enthusiastic pythonistas, from Kaggle fans to scientists. Well, that&rsquo;s not exactly true: notebooks are actually one of those things that you end up loving them or strongly feeling against them - with indifference being very rare and usually due to not having interacted with them a lot."/>

    <meta property="og:title" content="Grind those brains! Diffusion MRI preprocessing with papermill" />
<meta property="og:description" content="Notebooks: you love them or you hate them Notebooks are an interesting beast: they have become extremely diverse and popular, everyone loves them, from declared non-programmers to enthusiastic pythonistas, from Kaggle fans to scientists. Well, that&rsquo;s not exactly true: notebooks are actually one of those things that you end up loving them or strongly feeling against them - with indifference being very rare and usually due to not having interacted with them a lot." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://neurosnippets.com/posts/papermill-preproc/" />
<meta property="article:published_time" content="2021-02-17T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-02-17T00:00:00+00:00" />


  </head>
  <body>
    <header class="app-header">
      <a href="https://neurosnippets.com/"><img class="app-header-avatar" src="/img/me.png" alt="Matteo Mancini" /></a>
      <h1 class="title">NeuroSnippets</h1>
      <h3 class="author">Matteo Mancini</h3>
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/">Home</a>
             &bull; 
          
          <a class="app-header-menu-item" href="/tags/">Tags</a>
      </nav>
      <nav class="app-header-menu">
        
        
          
          
          <a class="app-header-menu-item" href="https://neurosnippets.com/about_me/">About me</a>
        
          
             &bull; 
          
          
          <a class="app-header-menu-item" href="https://neurosnippets.com/research/">Research</a>
        
      </nav>
      <p>A blog about brains, open-source code and (literally) everything in between.</p>
      <p><div class="question">Who am I</div> A biomedical engineer, Sir Henry Wellcome Postdoctoral Fellow, on a quest for better brain maps.</p>
      <div class="app-header-social">
        
          <a target="_blank" href="https://github.com/matteomancini" rel="noreferrer noopener"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg></a>
        
          <a target="_blank" href="https://twitter.com/ingmatman" rel="noreferrer noopener"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-twitter">
  <title>twitter</title>
  <path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"></path>
</svg></a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <a name="post"><h1 class ="post-title">Grind those brains! Diffusion MRI preprocessing with papermill</h1></a>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Feb 17, 2021
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          8 min read
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line>
</svg>
              <a class="tag" href="https://neurosnippets.com/tags/python/">python</a>
              <a class="tag" href="https://neurosnippets.com/tags/fsl/">fsl</a>
              <a class="tag" href="https://neurosnippets.com/tags/mrtrix3/">MRtrix3</a>
              <a class="tag" href="https://neurosnippets.com/tags/dmri/">dMRI</a>
        </div>
      </div>
    </header>
    
      <figure>
        <img class="post-pic" src="https://neurosnippets.com/img/papermill.svg">
        
          <figcaption>A sketch of a papermill-based workflow. Hopefully this approach saves you from <a href="https://docs.google.com/presentation/d/1n2RlMdmv1p25Xy5thJUhkKGvjtV-dkAIsUXP-AL4ffI/edit#slide=id.g3d168d2fd3_0_159" title="Joel Grus slides">American Chopper-style discussions</a>.</figcaption>
        
        </figure>
    
    <div class="post-content">
      <h2 id="notebooks-you-love-them-or-you-hate-them">Notebooks: you love them or you hate them</h2>
<p>Notebooks are an interesting beast: they have become extremely <a href="https://pg.ucsd.edu/publications/computational-notebooks-design-space_VLHCC-2020.pdf" title="The Design Space of Computational Notebooks: An Analysis of 60 Systems in Academia and Industry"><em>diverse</em></a> and popular, everyone loves them, from declared non-programmers to enthusiastic pythonistas, from Kaggle fans to scientists. Well, that&rsquo;s not exactly true: notebooks are actually one of those things that you end up loving them or strongly feeling <em>against</em> them - with indifference being very rare and usually due to not having interacted with them a lot. Although I tend to the <em>lovers side</em>, I can see why people get frustrated with some of their issues, especially the <em>hidden states</em> (for more about issues of notebooks, check the presentation from <a href="https://joelgrus.com">Joel Grus</a> at JupyterCon 2018 - video <a href="https://www.youtube.com/watch?v=7jiPeIFXb6U">here</a> and slide deck <a href="https://docs.google.com/presentation/d/1n2RlMdmv1p25Xy5thJUhkKGvjtV-dkAIsUXP-AL4ffI/edit#slide=id.g362da58057_0_1">here</a>). Despite these issues, even haters admit that the way notebooks allow to <em>mix and match</em> markdown, code and figures can have interesting applications. One of these applications, pioneered at <a href="https://netflixtechblog.com/notebook-innovation-591ee3221233" title="Netflix Tech Blog">Netflix</a>, is using them to process, report and troubleshoot at once (<em>*gasp*</em>) in production. It may sounds crazy, but if we think about it for a moment it makes sense: if we could avoid the hidden state issues running the notebooks as scripts, we end up with an object that after being executed will contain results, plots and error messages! But how can we do that? With <a href="https://papermill.readthedocs.io/en/latest/" title="Papermill Documentation"><code>papermill</code></a>!</p>
<p>In this post, I will show how to put together a <em>parameterized</em> notebook for diffusion MRI preprocessing and how to use <code>papermill</code> to run the notebook on a given dataset. I will assume that the input data are volumes acquired in anterior-posterior (AP) and posterior-anterior (PA) directions, without fixing any of those as the main direction yet. Using parameters, we will also be able to handle different readout times for the two directions, and an arbitraty order of the gradient values.
The goal is putting together a tool that is still easy to run as a <code>bash</code> script would be, but that provides also plots (to check intermediate results) and, in case of errors, the <em>environment</em> itself for easier troubleshooting - basically a script that turns into a report! Both the notebook and the related script are on the <a href="https://github.com/matteomancini/neurosnippets/tree/master/workflows/papermill-preproc">NeuroSnippets repository</a>.</p>
<h2 id="crushing-hidden-states-parameterized-notebooks">Crushing hidden states: parameterized notebooks</h2>
<p>A <em>parameterized</em> notebook is pretty much the same thing as an ordinary notebook, except for handling parameters in specific points, as we will see very shortly. One important advantage though is that they are thought to be run sequentially, avoiding most of the weird out-of-order execution issues. As we will see in the last section, <code>papermill</code>, an amazing Python package, takes care both of handling the parameters and executing them.</p>
<p>Let&rsquo;s start by importing the packages we will need (<code>nibabel</code> and <code>matplotlib</code>) and by defining a couple of tailored plotting functions:</p>
<pre><code>
import nibabel as nib
import matplotlib.pyplot as plt

def show_mid_slices(image):
    &quot;&quot;&quot; Function to display row of image middle slices &quot;&quot;&quot;
    shape = image.shape
    slices = [image[int(shape[0]/2), :, :],
             image[:, int(shape[1]/2), :],
             image[:, :, int(shape[2]/2)]]
    
    fig, axes = plt.subplots(1, len(slices), figsize=(500,200))
    for i, slice in enumerate(slices):
        axes[i].imshow(slice.T, cmap=&quot;gray&quot;, origin=&quot;lower&quot;)

        
def show_vol_slices(image):
    &quot;&quot;&quot; Function to display slices from several volumes &quot;&quot;&quot;
    shape = image.shape
    vols = shape[3]
    slices = image[:, :, int(shape[2]/2), :]
    
    fig, axes = plt.subplots(int(vols/5)+1, 5, figsize=(50,20*(int(vols/5)+1)))
    print((500,200*(int(vols/5)+1)))
    for i in range(vols):
        if vols &gt; 5:
            axes[int(i/5),i%5].imshow(slices[:,:,i].T, cmap=&quot;gray&quot;, origin=&quot;lower&quot;)
        else:
            axes[i].imshow(slices[:,:,i].T, cmap=&quot;gray&quot;, origin=&quot;lower&quot;)
    
</code></pre><p>The first function, slightly adapted from a similar one provided in the <a href="https://nipy.org/nibabel/coordinate_systems.html" title="Nibabel"><code>nibabel</code> tutorials</a>, gives an idea of how a volume looks like showing the middle slices in each of the three views; the second one instead shows a single slice (in this case an axial one) for each volume in a 4D dataset. So far, there&rsquo;s nothing different from a normal notebook. Let&rsquo;s see what&rsquo;s next:</p>
<pre><code>
# pa and ap parameters are mandatory
pa = &quot;&quot;
ap = &quot;&quot;
# bvec and bval need to be specified only for the main encoding direction
# and only if the basename is different from the nifti file
bvec = &quot;&quot;
bval = &quot;&quot;
# the main encoding direction (PA or AP)
main_dir = &quot;PA&quot;
# the readout may be omitted if it is the same for both directions
readout_pa = 0.1
readout_ap = 0.1
# the position of the actual b0 volumes
pa_b0 = 0
ap_b0 = 0

</code></pre><p>Here is the important part for <code>papermill</code>: in this cell we define the parameters for the notebook, the ones that we can change when running <code>papermill</code>. To make clear that this is the cell of the parameters, we will need to add the tag <code>parameters</code>
(to show the tags of each cell, we need to enable them by going through the menu: <code>View-&gt;Cell Toolbar-&gt;Tags</code>). For the mandatory parameters (the actual diffusion data), the default value is an empty string - this will make the notebook failing immediately if those are not provided. For the rest of them, it may not be necessary to change them when running <code>papermill</code> - it depends on the data. We now need to handle the details about the main encoding direction of the data and the gradient directions and values:</p>
<pre><code>if main_dir == &quot;PA&quot;:
    data = pa
    ix = 1
else:
    data = ap
    ix = 2
    
if bvec == &quot;&quot;:
    bvec = '/'.join([*data.split('/')[:-1], '']) + data.split('/')[-1].split('.')[0] + '.bvec'
    bval = '/'.join([*data.split('/')[:-1], '']) + data.split('/')[-1].split('.')[0] + '.bval'
    
</code></pre><p>First, we are keeping track of which data are related to the main direction (default is <code>PA</code>), and towards the end of the notebook we will take advantage of that. Then we are playing a bit with strings to allow us some additional flexibility: in this way, if the gradient data share the basename with the diffusion data, we don&rsquo;t have to specify those when we call <code>papermill</code>.</p>
<p>Now that we <em>parameterized</em> the notebook, we can continue with the visualization and the preprocessing. Let&rsquo;s start with having a look at a couple of b0 volumes from the original data:</p>
<pre><code>
pa_img = nib.load(pa)
pa_img_data = pa_img.get_fdata()

show_mid_slices(pa_img_data[:,:,:,pa_b0]) 

ap_img = nib.load(ap)
ap_img_data = ap_img.get_fdata()

show_mid_slices(ap_img_data[:,:,:,ap_b0])

</code></pre><p>We can now proceed with the first preprocessing steps:</p>
<pre><code>
%%bash -s &quot;$pa&quot; &quot;$ap&quot;
dwidenoise $1 PA_denoised.nii
mrdegibbs PA_denoised.nii PA_unringed.nii
dwidenoise $2 AP_denoised.nii
mrdegibbs AP_denoised.nii AP_unringed.nii

</code></pre><p>Here we are using <code>dwidenoise</code> and <code>mrdegibbs</code> from <a href="https://mrtrix.readthedocs.io/en/latest/" title="MRtrix3 Documentation">MRtrix3</a>. The easiest way to use these tools is directly from the shell, so I used one of the Jupyter <em>magic</em> tricks to write directly some <code>bash</code> lines, sending over the values from a couple of variables defined on the Python side. At this point, we can visualize again how things look if we want. After that, we can go with <code>topup</code> from <a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/" title="FSL website">FSL</a>:</p>
<pre><code>
%%bash -s &quot;$pa_b0&quot; &quot;$ap_b0&quot; &quot;$readout_pa&quot; &quot;$readout_ap&quot;
fslroi PA_unringed.nii b0_blip_up.nii.gz $1 1
fslroi AP_unringed.nii b0_blip_down.nii.gz $2 1
fslmerge -t b0_blip_up_down.nii.gz b0_blip_up.nii.gz b0_blip_down.nii.gz

printf &quot;0 1 0 $3\n0 -1 0 $4&quot; &gt; params.txt

topup --imain=b0_blip_up_down --datain=params.txt --config=b02b0.cnf --out=topup_results --iout=hifi

fslmaths hifi -Tmean hifi
bet hifi hifi_brain -m

</code></pre><p>Once again, it is a <code>bash</code> snippet with some <em>magic</em>. After visualizing the result, the last step left is <code>eddy</code> (from <a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/" title="FSL website">FSL</a> again):</p>
<pre><code>
%%bash -s &quot;$data&quot; &quot;$ix&quot; &quot;$bvec&quot; &quot;$bval&quot;
vols=$(mrinfo $1 | grep &quot;Dimensions&quot; | cut -d 'x' -f 4 | tr -d ' ')
indx=&quot;&quot;
for  ((i=1; i&lt;=$vols; i+=1)); do indx=&quot;$indx $2&quot;; done
echo $indx &gt; index.txt

eddy --imain=$1 --mask=hifi_brain_mask --acqp=params.txt --index=index.txt \
    --bvecs=$3 --bvals=$4 --topup=topup_results --out=eddy_corrected --data_is_shelled

</code></pre><p>The important thing to notice here is that we are running <code>eddy</code> on the data acquired along the main direction, and it is easy to know which one is it because of the if-else statement at the beginning. We are done with the pre-processing! The very last step is visualizing the final result:</p>
<pre><code>
eddy_img = nib.load('eddy_corrected.nii.gz')
eddy_img_data =eddy_img.get_fdata()

show_vol_slices(eddy_img_data)

</code></pre><h2 id="turning-the-mill">Turning the mill</h2>
<p>So we have a <em>parameterized</em> notebook ready to be run through <code>papermill</code>, but how does that work? First, we need some data. For this example, I chose the <a href="https://openneuro.org/datasets/ds003416/versions/1.0.0" title="MASiVar dataset">MASiVar dataset</a> shared by Leon Cai and colleagues on <a href="https://openneuro.org" title="OpenNeuro">OpenNeuro</a>, which includes non-preprocessed diffusion data for several subjects and several sessions (and several scanners!). The dataset can be easily downloaded through <a href="https://www.datalad.org" title="DataLad">DataLad</a>:</p>
<pre><code>datalad install https://github.com/OpenNeuroDatasets/ds003416.git
datalad get -J 4 ds003416/sub-cIIIsA01/ses-s1Bx1/*

</code></pre><p>As usual, to make things easier let&rsquo;s copy the data we want to process in a dedicated folder:</p>
<pre><code>mkdir sample_data
cp ds003416/sub-cIIIsA01/ses-s1Bx1/dwi/*104* sample_data/
cp ds003416/sub-cIIIsA01/ses-s1Bx1/dwi/*105* sample_data/
mkdir results
cd results

</code></pre><p>We have everything we need. It&rsquo;s <code>papermill</code> time!</p>
<pre><code>papermill ../dwi_preproc.ipynb \
    -p ap ../sample_data/sub-cIIIsA01_ses-s1Bx1_acq-b1000n3r21x21x22peAPA_run-104_dwi.nii.gz \
    -p pa ../sample_data/sub-cIIIsA01_ses-s1Bx1_acq-b1000n40r21x21x22peAPP_run-105_dwi.nii.gz \
    -p ap_b0 3 results.ipynb
</code></pre><p>Let&rsquo;s have a look at the <em>anatomy</em> of a <code>papermill</code> call: first, we pass the notebook we created; then, we pass all the parameters we want to change - in this case, in addition to the data filenames, I also speicify the position of the b0 volume for the AP dataset (we don&rsquo;t need to for the PA one, since it is at the default position 0); finally, we pass another notebook filename - the one that will be run with the specified parameters.</p>
<p>The final touch: if we are going to preprocess a lot of data, we probably don&rsquo;t want to open Jupyter and have a look at all the notebooks. To have a handy way to double-check that everything looks ok, we can convert the resulting notebook to HTML - creating our very own pre-processing report!</p>
<pre><code>jupyter nbconvert results.ipynb --no-input --to html

</code></pre><h2 id="useful-references">Useful references</h2>
<ul>
<li><a href="https://papermill.readthedocs.io/en/latest/" title="Papermill Documentation">Papermill</a></li>
<li><a href="https://netflixtechblog.com/notebook-innovation-591ee3221233" title="Netflix Tech Blog">How notebooks are used at Netflix</a></li>
<li><a href="https://openneuro.org/datasets/ds003416/versions/1.0.0" title="OpenNeuro">MASiVar dataset</a></li>
<li><a href="https://www.biorxiv.org/content/10.1101/2020.12.03.408567v1" title="bioRxiv">&ldquo;MASiVar: Multisite, Multiscanner, and Multisubject Acquisitions for Studying Variability in Diffusion Weighted Magnetic Resonance Imaging&rdquo;</a></li>
</ul>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
